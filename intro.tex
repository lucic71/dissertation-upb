\section{Context and motivation}

\todo{add a more general paragraph about c, ub and optimizations}
\todo{decide on which aspect you want to focus, either semantics of the
code or the intention of the progammer. the first gives a more formal
perspective of the problem which i dont think i want to tackle, the
secnod gives a more relaxed view of the problem, however i dont know how
to do science with that. i mean, how do you measure "the intention of
the programmer"?}

The C90~\cite{iso90} standard provides an ambiguous definition of
undefined behavior. This permits compiler implementations to abuse the
definition and use it as a mean to issue aggressive optimizations that
break non-trivial pieces of code in real-life software. Code that works
with a lower level of optimization is broken when the optimization level
is elevated. Furthermore code that works on previous versions of the
compiler is suddenly broken in newer versions because the standard
imposes virtually no requirements on undefined behavior.

\todo{add the other mitre integer overflow vulnerabilities from
blackhat}
This has created serious security problems throughout the
years~\cite{wang2012undefined,checks2008,funnull,mitre20200452}. A
number of initiatives to solve this problem were started from different
parties~\cite{google2015,regehr2014,wang2013towards,d2015correctness}
however the problem still persists. The primary open source programmer
groups have seized the unsteady definition of undefined behavior to
justify dangerous silent code optimizations that break code from
operating systems and other types of software projects.

An historical example of UB optimization can be found in
Listing~\ref{lst:junk}. srandomdev(3) is a common function used in BSD
systems for random number generation. It makes use of uninitialized
memory for generating randomness. This is a typical case of undefined
behavior and its consequences are best seen in the difference of the
generated code by two different compilers.

\begin{lstlisting}[style=Cstyle, caption={srandom function in
lib/libc/stdlib/random.c on BSD systems}, label={lst:junk}]
void
srandomdev(void)
{
	...
	struct timeval tv;
	unsigned long junk;

	gettimeofday(&tv, NULL);
	srandom((getpid() << 16) ^ tv.tv_sec ^ tv.tv_usec ^ junk);
	...
}
\end{lstlisting}

By inspecting \path{/usr/lib/libSystem.B.dylib} in Mac OS X 10.6 we see
the following generated code for Listing~\ref{lst:junk}:
\begin{lstlisting}[style=Cstyle, caption={}, label={lst:junk}]
leaq    0xe0(%rbp),%rdi
xorl    %esi,%esi
callq   0x001422ca      ; symbol stub for: _gettimeofday
callq   0x00142270      ; symbol stub for: _getpid
movq    0xe0(%rbp),%rdx
movl    0xe8(%rbp),%edi
xorl    %edx,%edi
shll    $0x10,%eax
xorl    %eax,%edi
xorl    %ebx,%edi
callq   0x00142d68      ; symbol stub for: _srandom
\end{lstlisting}

The generated code keeps the assumption of the programmer intact and the
compiler does not reason about the optimizations it can trigger on the
corresponding code. In the next version of the same operating system,
i.e. Mac OS X 10.7, the same file has the following generated code:

\begin{lstlisting}[style=Cstyle, caption={}, label={lst:junk}]
leaq    0xd8(%rbp),%rdi
xorl    %esi,%esi
callq   0x000a427e      ; symbol stub for: _gettimeofday
callq   0x000a3882      ; symbol stub for: _getpid
callq   0x000a4752      ; symbol stub for: _srandom
\end{lstlisting}

The newer version discarded the seed computation as an optimization,
generating code that calls gettimeofday and getpid but not using their
values. Furthermore srandom is called with some garbage value that does
not depend on the expression declared in the corresponding C code.

This change in the generated source code happened at approximately the
same time at which Apple decided to not use GCC anymore due to license
problems and switched to Clang/LLVM. Meanwhile, BSD systems solved this
problem and srandomdev used defined behavior for generating random
nubmers.

This philosophy is very dangerous in terms of programming expressivity.
The compiler has very little context of what the programmer wants to
accomplish with a specific piece of code. As we saw in the above
example, the compiler makes the assumption that using an unitialized
piece of memory makes no sense and discards the expression inside
srandom. On the same note the compiler cannot make the distinction
between a read from an uninitialized memory region that might produce
unspecified results and a read from a memory mapped device that cannot
be written in order to initialize it. Or it cannot distinguish between
an erroneous floating pointer access to an integer variable and a smart
method of computing an arithmetic function~\cite{lomont2003fast}. The
general principle is that the programmer has the responsibility to
decide what the code should do, the job of the compiler is to translate
the code into machine readable instructions and to apply optimizations
only on code that has defined behavior.

The argument of the people that defend this UB optimizations is
that C code that contains undefined behavior has no meaning and the
compiler is free to do various types of modifications on it. In Control
Theory terms, such a system is described by a low degree of
controllability and observability. Which is paradoxical in the
philosophy described above where the compiler forcefully takes the
responsibility, from the programmer, of generating relevant code. The
implications of this are that no meaningful engineering can be done in
this framework where processes inside a compiler cannot be understood
and analyzed.

The situation has created an adversarial view of the compiler.
Programmers are forced to modify the default behavior of the compiler
with flags such as -fno-delete-null-checks, -ftrapv,
-fno-strict-aliasing, -ftrapv so that they can impose strong
requirements on the code generated by the compiler. This in turn creates
a more complicated development environment.

Another argument that defends the aggressive optimizations view is that
code generated by these compilers runs faster on artificial
benchmarks~\cite{gcc10benck,gcc11bench}. This does not necessarily hold
for real-life software projects that differ in complexity from the
artificial benchmarks and that make use of non-trivial code constructs.
Ertl~\cite{ertl2015every} makes an interesting observation regarding the
performance of UB optimizations. He notes that source level changes buy
greater speedup factors than UB optimizations for certain classes
of programs. While his research in this field is valuable, the
limitation of his work is that he draws conclusions based on SPECint
benchmarks. This type of benchmarks provide code with defined behavior.

The contribution of this work is that we analyze the speedup factors of
UB optimizations for real-life software projects, such as operating
systems, in particular OpenBSD. We choose OpenBSD because it is a
self-contained robust and secure implementation of operating system.
The system emphasizes "portability, standardization, correctness,
proactive security [...]"~\cite{obsdmainp}, goals that are shared with
our philosophy of generating code that does not make assumptions about
undefined behaviors.

Finally, by running this experiment we provide a trade off
analysis between the performance gained using UB optimizations and the
risks of issuing them. Performance in this context has two meanings. On
one hand we are interested in seeing how fast the code is running and on
the other hand we are interested in the size of the generated code.

This paper is structured as follows. Section~\ref{sec:bg} presents a
background on UB optimizations with examples, risks and incomplete
solutions for solving the risk they introduce. Section~\ref{sec:rl}
describes previous results in analyzing the performance gain of UB
optimizations.  Section~\ref{sec:rp} introduces our research plan that
focuses on removing UB from OpenBSD and analyzing the performance of the
system. Finally, Section~\ref{sec:ccl} summarizes our research
proposal.
