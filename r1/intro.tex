\section{Context and motivation}

The ISO C Standard~\cite{iso90} provides a definition of undefined
behavior that gives absolute freedom to compiler implementation when
erroneous program constructs, erroneous data or indeterminately-valued
objects are encountered. This allows various compilers to treat
undefined behavior in different ways while still being standard
conformant. For example, signed overflow is undefined behavior as there
are multiple ways of treating it based on the target architecture. On
MIPS and DEC Alpha the ADD and ADDV instructions trap while on IA-32 the
ADD instruction performs second's complement wrapping.

The freedom that undefined behavior supplies also gives birth to
compiler opitmizations. The main philosophy here is that a program that
triggers undefined behavior for some input is incorrect. The compiler
can assume that the program it is gives is always correct and so, it
takes advantage of undefined behavior by assuming it never happens. The
following piece of code:~\textit{\(if (a + c < a + b)\)} can be
transformed, in this case, into the following piece of
code:~\textit{\(if (c < b)\)}, assuming that the addition is signed.
This is possible because signed oveflow is undefined, as discussed, and
the compiler is free to infer the addition property of inequality.

Certain classes of programs, such as High Performance Computing take
great advantage of undefined behavior for issuing compiler
optimizations. Programs in this class contain tight loops that perform a
a significant amount of operations per second. As a consqeuence
programers are interested in generating code that runs as fast as
possible. Not needing to perform bounds checks, not considering the case
where the signed iterator of the loop oveflows or converting the loop
iterator from signed to unsigned are just a few scenarios that can boost
the performance of the code.

Wang et al.~\cite{wang2012undefined} experimented with the consequences
of disabling undefined behavior optimizations on the SPECint 2006
benchmark. Out of 12 programs in the benchmark they noticed slowdowns
for 2 of them. 456.hmmer slows down $7.2\%$ with GCC 4.7 and $9.0\%$
with Clang/LLVM 3.1 while 462.libquantum slows down $6.3\%$ with the
same version of GCC and $11.8\%$ with the same version of Clang/LLVM.

In Operating Systems things behave differently than in the case of High
Performance Computing. Operating Systems are part of a class of programs
that are not CPU bound hence might not benefit from the performance
advantages of undefined behavior. Instead they make part of the class of
programs where security and robustness play an important role. Because
of this, undefined behavior optimizations must be carefully analyzed so
that they do not introduce security issues, as Linus Torvalds
noted~\cite{linusgcc}:
\begin{displayquote}
Performance doesn't come from occasional small and odd
micro-optimizations. I care about performance a lot, and I actually look
at generated code and do profiling etc. None of those three options
[-fno-strict-overflow, -fno-strict-aliasing,
-fno-deletel-null-pointer-checks] have *ever* shown up as issues. But
the incorrect code they generate? It has.
\end{displayquote}

The problems Torvalds complains about are often encountered in the field
of Operating Systems. One of the most well known security issue found
due to undefined behavior optimizations was found in BSD systems.
Listing~\ref{lst:junk} presents a historical snippet of code from
srandomdev(3), a function that initializes the random number generator
of the system.

\begin{lstlisting}[style=Cstyle, caption={srandom function in
lib/libc/stdlib/random.c on BSD systems}, label={lst:junk}]
void
srandomdev(void)
{
	...
	struct timeval tv;
	unsigned long junk;

	gettimeofday(&tv, NULL);
	srandom((getpid() << 16) ^ tv.tv_sec ^ tv.tv_usec ^ junk);
	...
}
\end{lstlisting}

It makes use of unitialized memory, through junk, for generating
randomness. This is a typical case of undefined behavior and its
consequences are best seen in the difference of the generated code by
two different compilers.

By inspecting \path{/usr/lib/libSystem.B.dylib} in Mac OS X 10.6 we see
the following generated code for Listing~\ref{lst:junk}:
\begin{lstlisting}[style=Cstyle, caption={}, label={lst:junk}]
leaq    0xe0(%rbp),%rdi
xorl    %esi,%esi
callq   0x001422ca      ; symbol stub for: _gettimeofday
callq   0x00142270      ; symbol stub for: _getpid
movq    0xe0(%rbp),%rdx
movl    0xe8(%rbp),%edi
xorl    %edx,%edi
shll    $0x10,%eax
xorl    %eax,%edi
xorl    %ebx,%edi
callq   0x00142d68      ; symbol stub for: _srandom
\end{lstlisting}

The compiler allocates a slot on the stack for junk and interprets that
slot as the value of the junk variable that will be used in the srandom
argument computation. 

In the next version of the same operating system, i.e. Mac OS X 10.7,
the same file has the following generated code:
\begin{lstlisting}[style=Cstyle, caption={}, label={lst:junk}]
leaq    0xd8(%rbp),%rdi
xorl    %esi,%esi
callq   0x000a427e      ; symbol stub for: _gettimeofday
callq   0x000a3882      ; symbol stub for: _getpid
callq   0x000a4752      ; symbol stub for: _srandom
\end{lstlisting}

The newer version discards the seed computation as an optimization,
generating code that calls gettimeofday and getpid but not using their
values. Furthermore srandom is called with some garbage value that does
not depend on the expression declared in the corresponding C code.

This change in the generated source code happened at approximately the
same time at which Apple decided to not use GCC anymore due to license
problems and switched to Clang/LLVM. Meanwhile, BSD systems solved this
problem and srandomdev(3) uses defined behavior for generating random
numbers~\cite{junkobsd,junkfbsd,junkdbsd}.

In this context, it is important to analyze for each class of programs
what are the advantages and the disadvantages of undefined behavior
optimizations issued by the compiler based on the requirements of the
class. Wang et al.~\cite{wang2012undefined} started the work in this
field by analyzing the preformance of GCC and Clang with a limited set
of disabled undefined behavior optimizations on SPECint 2006 benchmark.

We take their work one step further and analyze the advantages and
disadvantages of undefined behavior optimizations for real-life
software.

To achieve our goals, we take Clang/LLVM and disable all undefined
behavior optimizations, e.g signed overflow optimizations, null access
optimizaitions, oversized shift optimizations, etc. We do the
modifications either through the already accessible compiler flags
or by changing the internals of the compiler. The end result will be a
modified compiler free of undefined behavior optimizations that will be
used to test the advantages and disadvantages for each class of
programs.

The metric we use for assessing the advantages and disadvantages is
performance. The performance may include, but it is not limited to, code
size and code speed. We take examples from each class of programs and
create benchmarks that show how the performance has changed based on the
modifications done in the compiler.

This study is structured as follows. [needs reformulaiton]
