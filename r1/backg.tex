\section{Background} \label{sec:bg}

This section presents two examples of undefined behavior optimizations,
i.e. signed overflow optimizations and null access optimizations.

\subsection{Signed Overflow Optimizations}
\todo{talk about that mail discussed with Nuno about nsw}
The signed integer overflow optimization is highly used in compilers
nowdays. C compilers are unable to generate fast code for
architectures where the size of int is different from the register
width. However, for backwards compatibility reasons, int is still 32 bit
on all 64 bit major architectures. This creates problems when int is
used for generating memory addresses and for generating memory accesses.

The issue is fairly common when generating code for loop variables.
Given the following piece of code:
\begin{lstlisting}[style=Cstyle, caption={}, label={}]
sum = 0;
for (int i = 0; i < count; ++i)
	sum += x[i];
\end{lstlisting}
the compiler can naively generate the following piece of assembly code:
\begin{lstlisting}[style=Cstyle, caption={}, label={}]
                            ; ecx = count
                            ; rsi = points to x[]
    xor    eax, eax         ; clear sum
    xor    ebx, ebx         ; i
  lp:
    movsxd rdx, ebx         ; sign-extend i to 64 bits
    add    eax, [rsi+rdx*4] ; sum += x[i]
    inc    ebx              ; i++
    cmp    ebx, ecx         ; if i < count
    jl     lp               ;   goto lp
  done:
\end{lstlisting}

Because the loop counter is a 32 bit value it needs to be sign extended
to 64 bits in order to be able to access the memory at x[i]. Things
complicate when the compiler is free to loop unroll the computation
inside the loop:
\begin{lstlisting}[style=Cstyle, caption={}, label={}]
  lp:
    movsxd rdi, ebx         ; sign-extend i to 64 bits
    add    eax, [rsi+rdi*4] ; sum += x[i]
    lea    edi, [ebx+1]     ; i+1
    movsxd rdi, edi         ; sign-extend i+1 to 64 bits
    add    eax, [rsi+rdi*4] ; sum += x[i+1]
    lea    edi, [ebx+2]     ; i+2
    movsxd rdi, edi         ; sign-extend i+2 to 64 bits
    add    eax, [rsi+rdi*4] ; sum += x[i+2]
    lea    edi, [ebx+3]     ; i+3
    movsxd rdi, edi         ; sign-extend i+3 to 64 bits
    add    eax, [rsi+rdi*4] ; sum += x[i+3]
    add    ebx, 4           ; i=i+4
\end{lstlisting}

Here, the compiler is free to do loop unrolling but because the variable
is declared on 32 bits it needs to extend it to 64 bits every time it
wants to use it. An alternative would be to sign extend i only one time
and them use the 64 bit value for the further computations but for this
to be possible it is neccesary that the compiler proves that i will
never overflow.

However we can use the fact that signed overflow is undefined and
promote the int variable to a int64 type. The resulting C could will
look as follows:
\begin{lstlisting}[style=Cstyle, caption={}, label={}]
sum = 0;
int64 count64 = (int64) count;
for (int64 i = 0; i < count64; ++i)
	sum += x[i];
\end{lstlisting}

In this situation the unrolled code can be transformed into:
\begin{lstlisting}[style=Cstyle, caption={}, label={}]
  lp:
    add    eax, [rsi+0]    ; sum += x[i+0]
    add    eax, [rsi+4]    ; sum += x[i+1]
    add    eax, [rsi+8]    ; sum += x[i+2]
    add    eax, [rsi+12]   ; sum += x[i+3]
    add    rsi, 16         ; x += 4
\end{lstlisting}

In this situation the width of the loop counter is equal to the width of
the pointer and the compiler is free to perform further optimizations
that take advantage of pointer arithmetic.

\subsection{NULL Access Optimizations}
Another class of undefined behavior optimizations is based on the idea
that NULL accesses are not defined by the standard. GCC was the first to
take advantage of this using -fdelete-null-pointer-check. By using
global dataflow analysis the compiler can eliminate useless checks for
null pointer. If a pointer is checked after it was dereferenced, it
cannot be NULL.

However some environments depend on the assumption that dereferencing a
NULL pointer does not cause a problem. This has caused a security
vulnerability in the Linux kernel, more specifically in the tun_chr_poll
function shown in Listing~\ref{lst:tun_chr_poll}.
\begin{lstlisting}[style=Cstyle, caption={tun_chr_poll in
drivers/net/tun.c of the Linux kernel}, label={lst:tun_chr_poll}]
unsigned int
tun_chr_poll(struct file *file, poll_table * wait)
{
	struct tun_file *tfile = file->private_data;
	struct tun_struct *tun = __tun_get(tfile);
	struct sock *sk = tun->sk;
	if (!tun)
		return POLLERR;
	...
}
\end{lstlisting}

The compilers notices that the tun pointer is dereferenced before it is
NULL-checked and as an optimization it deletes the NULL check. If the
memory page where NULL is present is not mapped then the kernel will
generate a segmentaiton violation and it will halt. This happens
regardless of the deleted NULL check. However if the NULL check is
deleted and the page that contains NULL is mapped then the function does
neither return a POLLERR nor generates a segmentation violation. In this
point the system is vulnerable as an attacker could inject dangerous
information in the page where NULL is present.
